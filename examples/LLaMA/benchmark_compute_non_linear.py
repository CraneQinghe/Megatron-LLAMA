import torch
import time
import argparse

def benchmark_linear(input_shape, weight_shape, max_iters=200, warmup=50):
    """
    æ¨¡æ‹Ÿ PyTorch åº•å±‚çš„ nn.Linear(K, N) ä»¥åŠ F.linear(x, weight)
    x çš„å½¢çŠ¶æ˜¯ [M, K]
    weight çš„å½¢çŠ¶æ˜¯ [N, K] 
    è¾“å‡ºå½¢çŠ¶æ˜¯ [M, N]
    """
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åˆå§‹åŒ– Tensorï¼Œä½¿ç”¨ FP16 (å¦‚æœåœ¨ CPU/MPS ä¸Šå¯èƒ½éœ€è¦ fallback åˆ° FP32ï¼Œè¿™é‡Œè‡ªåŠ¨å¤„ç†ä¸€ä¸‹)
    dtype = torch.float16 if device == 'cuda' else torch.float32
    
    x = torch.randn(input_shape, device=device, dtype=dtype)
    w = torch.randn(weight_shape, device=device, dtype=dtype)
    
    # é¢„çƒ­é¢„çƒ­
    for _ in range(warmup):
        y = torch.matmul(x, w.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(max_iters):
        y = torch.matmul(x, w.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    end = time.time()
    
    avg_time_ms = (end - start) / max_iters * 1000.0
    
    # è®¡ç®—ç†è®º FLOPs: 2 * M * N * K
    M = input_shape[0] if len(input_shape) == 1 else input_shape[0] * (input_shape[1] if len(input_shape)>1 else 1)
    if len(input_shape) == 3: M = input_shape[0] * input_shape[1]
    K = weight_shape[1]
    N = weight_shape[0]
    
    flops = 2 * M * N * K
    tflops_per_sec = (flops / 1e12) / (avg_time_ms / 1000.0)
    
    return avg_time_ms, tflops_per_sec

def benchmark_sequential_layer(qkv_shapes, o_shapes, gu_shapes, down_shapes, max_iters=200, warmup=50):
    """
    å®Œå…¨æ¨¡æ‹Ÿ LLaMA å•ä¸ª Layer å†…éƒ¨å‰å‘ä¼ æ’­çš„ç®—å­ä¸²è¡Œæ‰§è¡Œé¡ºåºã€‚
    QKV -> O -> GateUp -> Down
    """
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    dtype = torch.float16 if device == 'cuda' else torch.float32
    
    # æå‰åˆ†é…å¥½æ‰€æœ‰çš„è¾“å…¥å’Œæƒé‡ï¼Œæ¨¡æ‹Ÿæ˜¾å­˜é©»ç•™
    x_qkv = torch.randn(qkv_shapes[0], device=device, dtype=dtype)
    w_qkv = torch.randn(qkv_shapes[1], device=device, dtype=dtype)
    
    x_o = torch.randn(o_shapes[0], device=device, dtype=dtype)
    w_o = torch.randn(o_shapes[1], device=device, dtype=dtype)
    
    x_gu = torch.randn(gu_shapes[0], device=device, dtype=dtype)
    w_gu = torch.randn(gu_shapes[1], device=device, dtype=dtype)
    
    x_down = torch.randn(down_shapes[0], device=device, dtype=dtype)
    w_down = torch.randn(down_shapes[1], device=device, dtype=dtype)
    
    for _ in range(warmup):
        _ = torch.matmul(x_qkv, w_qkv.t())
        _ = torch.matmul(x_o, w_o.t())
        _ = torch.matmul(x_gu, w_gu.t())
        _ = torch.matmul(x_down, w_down.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(max_iters):
        _ = torch.matmul(x_qkv, w_qkv.t())
        _ = torch.matmul(x_o, w_o.t())
        _ = torch.matmul(x_gu, w_gu.t())
        _ = torch.matmul(x_down, w_down.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    end = time.time()
    
    avg_total_time = (end - start) / max_iters * 1000.0
    return avg_total_time

def profile_llama_layer(S=4096, B=1, H=4096, FFN=11008, tp_sizes=[1, 2, 4, 8]):
    """
    æå– LLaMA æ ¸å¿ƒå¼ é‡åœ¨å„ç§ TP ç­–ç•¥åˆ‡åˆ†ä¸‹çš„å®½å’Œé«˜ï¼š
    æ³¨ï¼šMegatron åºåˆ—å¹¶è¡Œ(SP)åœ¨å‘ç”ŸçŸ©é˜µä¹˜å‰ï¼Œå·²ç»AllGatheræŠŠSeqé•¿åº¦é›†é½åˆ°äº† S
    æ‰€ä»¥å®é™…è¿›å…¥çŸ©é˜µä¹˜æ³•çš„è¡Œæ•° M = S * B æ˜¯é“å®šä¸å˜çš„ï¼å˜çš„æ˜¯æ¨ªå‘åˆ‡é™¤çš„åˆ—å®½ N å’Œ Kï¼
    """
    print(f"ğŸš€ Profiling LLaMA TensorCore Non-Linearity (B={B}, S={S}, Hidden={H}, FFN={FFN})")
    print("=" * 110)
    print(f"{'TP':<4} | {'Module':<12} | {'Time(ms)':<10} | {'TFLOPs/s':<10} | {'Input [M, K]':<18} x {'Weight [N, K]':<18}")
    print("-" * 110)
    
    for tp in tp_sizes:
        M = S * B
        
        # 1. Attention QKV æŠ•å½± (ColumnParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [3*H, H]ï¼Œè¢«åˆ‡åˆ†ä¸º [3*H/TP, H]
        qkv_time, qkv_tflops = benchmark_linear((M, H), (3 * H // tp, H))
        
        # 2. Attention O æŠ•å½± (RowParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [H, H]ï¼Œç”±äºè¾“å…¥ç‰¹å¾ä¹Ÿæ˜¯åˆ‡å¥½çš„ï¼Œæƒé‡è¢«åˆ‡åˆ†ä¸º [H, H/TP]
        o_time, o_tflops = benchmark_linear((M, H // tp), (H, H // tp))
        
        # 3. MLP Gate + Up æŠ•å½± (ä½¿ç”¨ SwiGLU å¸¸è§çš„åˆå¹¶ ColumnParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [2*FFN, H]ï¼Œè¢«åˆ‡åˆ†ä¸ºäº† [2*FFN/TP, H]
        gate_up_time, gu_tflops = benchmark_linear((M, H), (2 * FFN // tp, H))
        
        # 4. MLP Down æŠ•å½± (RowParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [H, FFN]ï¼Œè¢«åˆ‡åˆ†ä¸º [H, FFN/TP]
        down_time, d_tflops = benchmark_linear((M, FFN // tp), (H, FFN // tp))
        
        total_attn = qkv_time + o_time
        total_mlp = gate_up_time + down_time
        
        # ä¸²è¡Œåˆå¹¶è·‘ï¼ˆæµ‹é‡æ›´çœŸå®çš„ L2 Cache ç«äº‰ä¸ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€ï¼‰
        simulated_layer_time = benchmark_sequential_layer(
            ((M, H), (3 * H // tp, H)),
            ((M, H // tp), (H, H // tp)),
            ((M, H), (2 * FFN // tp, H)),
            ((M, FFN // tp), (H, FFN // tp))
        )
        
        print(f"{tp:<4} | {'Attn QKV':<12} | {qkv_time:<10.3f} | {qkv_tflops:<10.1f} | [{M:>5}, {H:>5}]       x [{3*H//tp:>5}, {H:>5}]")
        print(f"{tp:<4} | {'Attn O':<12} | {o_time:<10.3f} | {o_tflops:<10.1f} | [{M:>5}, {H//tp:>5}]       x [{H:>5}, {H//tp:>5}]")
        print(f"{tp:<4} | {'[Attn Total]':<12} | {total_attn:<10.3f} |")
        print(f"{tp:<4} | {'MLP GateUp':<12} | {gate_up_time:<10.3f} | {gu_tflops:<10.1f} | [{M:>5}, {H:>5}]       x [{2*FFN//tp:>5}, {H:>5}]")
        print(f"{tp:<4} | {'MLP Down':<12} | {down_time:<10.3f} | {d_tflops:<10.1f} | [{M:>5}, {FFN//tp:>5}]       x [{H:>5}, {FFN//tp:>5}]")
        print(f"{tp:<4} | {'[MLP Total]':<12} | {total_mlp:<10.3f} |")
        print(f"{tp:<4} | {'[LAYER REAL]':<12} | {simulated_layer_time:<10.3f} | {' '*10} | (Sequential Pipeline Simulation)")
        print("-" * 110)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--S', type=int, default=4096, help="Sequence Length")
    parser.add_argument('--B', type=int, default=1, help="Micro Batch Size")
    parser.add_argument('--H', type=int, default=4096, help="Hidden Size")
    parser.add_argument('--FFN', type=int, default=11008, help="FFN Intermediate Size")
    args = parser.parse_args()
    
    device_name = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"[*] Detected hardware device: {device_name.upper()}")
    if device_name != 'cuda':
        print("[!] è­¦å‘Š: æœ¬åœ°è¿è¡Œä»…æµ‹è¯•ä»£ç é€»è¾‘ã€‚æ¬²è§‚æµ‹çœŸå®çš„ TensorCore éçº¿æ€§æŠ˜æŸï¼Œè¯·å°†æ­¤è„šæœ¬æäº¤è‡³ A40/A100 GPU é›†ç¾¤è¿è¡Œï¼\n")
        
    profile_llama_layer(S=args.S, B=args.B, H=args.H, FFN=args.FFN, tp_sizes=[1, 2, 4, 8])
