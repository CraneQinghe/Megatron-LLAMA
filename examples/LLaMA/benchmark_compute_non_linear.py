import torch
import time
import argparse
import json
import os

def benchmark_linear(input_shape, weight_shape, max_iters=200, warmup=50):
    """
    æ¨¡æ‹Ÿ PyTorch åº•å±‚çš„ nn.Linear(K, N) ä»¥åŠ F.linear(x, weight)
    x çš„å½¢çŠ¶æ˜¯ [M, K]
    weight çš„å½¢çŠ¶æ˜¯ [N, K] 
    è¾“å‡ºå½¢çŠ¶æ˜¯ [M, N]
    """
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åˆå§‹åŒ– Tensorï¼Œä½¿ç”¨ FP16 (å¦‚æœåœ¨ CPU/MPS ä¸Šå¯èƒ½éœ€è¦ fallback åˆ° FP32ï¼Œè¿™é‡Œè‡ªåŠ¨å¤„ç†ä¸€ä¸‹)
    dtype = torch.float16 if device == 'cuda' else torch.float32
    
    x = torch.randn(input_shape, device=device, dtype=dtype)
    w = torch.randn(weight_shape, device=device, dtype=dtype)
    
    # é¢„çƒ­é¢„çƒ­
    for _ in range(warmup):
        y = torch.matmul(x, w.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(max_iters):
        y = torch.matmul(x, w.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    end = time.time()
    
    avg_time_ms = (end - start) / max_iters * 1000.0
    
    # è®¡ç®—ç†è®º FLOPs: 2 * M * N * K
    M = input_shape[0] if len(input_shape) == 1 else input_shape[0] * (input_shape[1] if len(input_shape)>1 else 1)
    if len(input_shape) == 3: M = input_shape[0] * input_shape[1]
    K = weight_shape[1]
    N = weight_shape[0]
    
    flops = 2 * M * N * K
    tflops_per_sec = (flops / 1e12) / (avg_time_ms / 1000.0)
    
    return avg_time_ms, tflops_per_sec

def benchmark_memory_bound(op_type, shape, max_iters=200, warmup=50):
    """
    æµ‹é‡çº¯è®¿å­˜ç®—å­ (LayerNorm, SiLU) çš„æ‰§è¡Œæ—¶é—´ã€‚
    """
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    dtype = torch.float16 if device == 'cuda' else torch.float32
    
    x = torch.randn(shape, device=device, dtype=dtype)
    
    if op_type == "rmsnorm":
        w = torch.ones(shape[-1], device=device, dtype=dtype)
        op = lambda: torch.nn.functional.layer_norm(x, (shape[-1],), weight=w)
    elif op_type == "silu":
        op = lambda: torch.nn.functional.silu(x)
        
    for _ in range(warmup):
        op()
        
    if device == 'cuda': torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(max_iters):
        op()
        
    if device == 'cuda': torch.cuda.synchronize()
    end = time.time()
    
    return (end - start) / max_iters * 1000.0

def benchmark_sequential_layer(qkv_shapes, o_shapes, gu_shapes, down_shapes, H, max_iters=200, warmup=50):
    """
    å®Œå…¨æ¨¡æ‹Ÿ LLaMA å•ä¸ª Layer å†…éƒ¨å‰å‘ä¼ æ’­çš„ 7 æ­¥ç®—å­ä¸²è¡Œæ‰§è¡Œé¡ºåºã€‚
    Norm -> QKV -> (å¿½ç•¥FlashAttné€šä¿¡) -> O -> Norm -> GateUp -> SiLU -> Down
    """
    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    dtype = torch.float16 if device == 'cuda' else torch.float32
    
    # æå‰åˆ†é…å¥½æ‰€æœ‰çš„è¾“å…¥å’Œæƒé‡ï¼Œæ¨¡æ‹Ÿæ˜¾å­˜é©»ç•™
    x_in = torch.randn(qkv_shapes[0], device=device, dtype=dtype)
    w_qkv = torch.randn(qkv_shapes[1], device=device, dtype=dtype)
    
    x_o = torch.randn(o_shapes[0], device=device, dtype=dtype)
    w_o = torch.randn(o_shapes[1], device=device, dtype=dtype)
    
    x_mlp = torch.randn(gu_shapes[0], device=device, dtype=dtype)
    w_gu = torch.randn(gu_shapes[1], device=device, dtype=dtype)
    
    # Fake weights for RMSNorm (LayerNorm approximation)
    rmsnorm_weight = torch.ones(H, device=device, dtype=dtype)
    
    w_down = torch.randn(down_shapes[1], device=device, dtype=dtype)
    
    for _ in range(warmup):
        # 1. Input RMSNorm
        norm1 = torch.nn.functional.layer_norm(x_in, (H,), weight=rmsnorm_weight)
        # 2. QKV
        qkv_out = torch.matmul(norm1, w_qkv.t())
        # 3. O_Proj (Pretend attention happened and we have x_o)
        o_out = torch.matmul(x_o, w_o.t())
        # 4. Post-Attention RMSNorm
        norm2 = torch.nn.functional.layer_norm(x_mlp, (H,), weight=rmsnorm_weight)
        # 5. Gate+Up å‡ç»´
        gu_out = torch.matmul(norm2, w_gu.t())
        # 6. SiLU (SwiGLU çš„éçº¿æ€§æ¿€æ´»)
        silu_out = torch.nn.functional.silu(gu_out)
        # 7. Down é™ç»´
        _ = torch.matmul(silu_out[..., :down_shapes[0][1]], w_down.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(max_iters):
        # 1. Input RMSNorm
        norm1 = torch.nn.functional.layer_norm(x_in, (H,), weight=rmsnorm_weight)
        # 2. QKV
        qkv_out = torch.matmul(norm1, w_qkv.t())
        # 3. O_Proj 
        o_out = torch.matmul(x_o, w_o.t())
        # 4. Post-Attention RMSNorm
        norm2 = torch.nn.functional.layer_norm(x_mlp, (H,), weight=rmsnorm_weight)
        # 5. Gate+Up å‡ç»´
        gu_out = torch.matmul(norm2, w_gu.t())
        # 6. SiLU 
        silu_out = torch.nn.functional.silu(gu_out)
        # 7. Down é™ç»´
        _ = torch.matmul(silu_out[..., :down_shapes[0][1]], w_down.t())
        
    if device == 'cuda': torch.cuda.synchronize()
    end = time.time()
    
    avg_total_time = (end - start) / max_iters * 1000.0
    return avg_total_time

def profile_llama_layer(S=4096, B=1, H=4096, FFN=11008, tp_sizes=[1, 2, 4, 8]):
    """
    æå– LLaMA æ ¸å¿ƒå¼ é‡åœ¨å„ç§ TP ç­–ç•¥åˆ‡åˆ†ä¸‹çš„å®½å’Œé«˜ï¼š
    æ³¨ï¼šMegatron åºåˆ—å¹¶è¡Œ(SP)åœ¨å‘ç”ŸçŸ©é˜µä¹˜å‰ï¼Œå·²ç»AllGatheræŠŠSeqé•¿åº¦é›†é½åˆ°äº† S
    æ‰€ä»¥å®é™…è¿›å…¥çŸ©é˜µä¹˜æ³•çš„è¡Œæ•° M = S * B æ˜¯é“å®šä¸å˜çš„ï¼å˜çš„æ˜¯æ¨ªå‘åˆ‡é™¤çš„åˆ—å®½ N å’Œ Kï¼
    """
    print(f"ğŸš€ Profiling LLaMA TensorCore Non-Linearity (B={B}, S={S}, Hidden={H}, FFN={FFN})")
    print("=" * 110)
    print(f"{'TP':<4} | {'Module':<12} | {'Time(ms)':<10} | {'TFLOPs/s':<10} | {'Input [M, K]':<18} x {'Weight [N, K]':<18}")
    print("-" * 110)
    
    results = {}
    
    for tp in tp_sizes:
        M = S * B
        
        # 1. Attention QKV æŠ•å½± (ColumnParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [3*H, H]ï¼Œè¢«åˆ‡åˆ†ä¸º [3*H/TP, H]
        qkv_time, qkv_tflops = benchmark_linear((M, H), (3 * H // tp, H))
        
        # 2. Attention O æŠ•å½± (RowParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [H, H]ï¼Œç”±äºè¾“å…¥ç‰¹å¾ä¹Ÿæ˜¯åˆ‡å¥½çš„ï¼Œæƒé‡è¢«åˆ‡åˆ†ä¸º [H, H/TP]
        o_time, o_tflops = benchmark_linear((M, H // tp), (H, H // tp))
        
        # 3. MLP Gate + Up æŠ•å½± (ä½¿ç”¨ SwiGLU å¸¸è§çš„åˆå¹¶ ColumnParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [2*FFN, H]ï¼Œè¢«åˆ‡åˆ†ä¸ºäº† [2*FFN/TP, H]
        gate_up_time, gu_tflops = benchmark_linear((M, H), (2 * FFN // tp, H))
        
        # 4. MLP Down æŠ•å½± (RowParallelLinear)
        # æƒé‡åŸæœ¬æ˜¯ [H, FFN]ï¼Œè¢«åˆ‡åˆ†ä¸º [H, FFN/TP]
        down_time, d_tflops = benchmark_linear((M, FFN // tp), (H, FFN // tp))
        
        total_attn = qkv_time + o_time
        total_mlp = gate_up_time + down_time
        
        # ç‹¬ç«‹æµ‹é‡è®¿å­˜ç®—å­çš„è€—æ—¶ (Seq Parallel ä¼šå¯¹åºåˆ— S è¿›è¡Œåˆ‡ç‰‡ï¼Œæ‰€ä»¥è¿™é‡Œçš„è¡Œæ•°æ˜¯ M/tp)
        norm_time = benchmark_memory_bound("rmsnorm", (M // tp, H))
        # SiLU å‘ç”Ÿåœ¨ Gate+Up ä¹‹åï¼Œè¢« TP åˆ‡åˆ†è¿‡
        silu_time = benchmark_memory_bound("silu", (M, 2 * FFN // tp))
        
        # ä¸²è¡Œåˆå¹¶è·‘ï¼ˆæµ‹é‡æ›´çœŸå®çš„ L2 Cache ç«äº‰ä¸ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€ï¼‰
        simulated_layer_time = benchmark_sequential_layer(
            ((M, H), (3 * H // tp, H)),
            ((M, H // tp), (H, H // tp)),
            ((M, H), (2 * FFN // tp, H)),
            ((M, FFN // tp), (H, FFN // tp)),
            H
        )
        
        print(f"{tp:<4} | {'[Memory]':<12} | {norm_time:<10.3f} | {'N/A (Bound)':<10} | RMSNorm (Input)")
        print(f"{tp:<4} | {'Attn QKV':<12} | {qkv_time:<10.3f} | {qkv_tflops:<10.1f} | [{M:>5}, {H:>5}]       x [{3*H//tp:>5}, {H:>5}]")
        print(f"{tp:<4} | {'Attn O':<12} | {o_time:<10.3f} | {o_tflops:<10.1f} | [{M:>5}, {H//tp:>5}]       x [{H:>5}, {H//tp:>5}]")
        print(f"{tp:<4} | {'[Attn Total]':<12} | {total_attn:<10.3f} |")
        print(f"{tp:<4} | {'[Memory]':<12} | {norm_time:<10.3f} | {'N/A (Bound)':<10} | RMSNorm (Post Attn)")
        print(f"{tp:<4} | {'MLP GateUp':<12} | {gate_up_time:<10.3f} | {gu_tflops:<10.1f} | [{M:>5}, {H:>5}]       x [{2*FFN//tp:>5}, {H:>5}]")
        print(f"{tp:<4} | {'[Memory]':<12} | {silu_time:<10.3f} | {'N/A (Bound)':<10} | SiLU (Swish Activation)")
        print(f"{tp:<4} | {'MLP Down':<12} | {down_time:<10.3f} | {d_tflops:<10.1f} | [{M:>5}, {FFN//tp:>5}]       x [{H:>5}, {FFN//tp:>5}]")
        print(f"{tp:<4} | {'[MLP Total]':<12} | {total_mlp:<10.3f} |")
        
        sum_of_parts = norm_time + total_attn + norm_time + total_mlp + silu_time
        print(f"{tp:<4} | {'[Sum Parts]':<12} | {sum_of_parts:<10.3f} | {' '*10} | (Math Sum of 7 Operators)")
        print(f"{tp:<4} | {'[LAYER REAL]':<12} | {simulated_layer_time:<10.3f} | {' '*10} | (Sequential Pipeline Simulation)")
        print("-" * 110)
        
        # å°†å„ç®—å­çš„è€—æ—¶ä¸å®ƒä»¬çš„å…·ä½“è¾“å…¥ã€è¾“å‡ºã€æƒé‡å½¢çŠ¶ç»‘å®šèµ·æ¥ä½œä¸º JSON key
        # æ ¼å¼: ç®—å­åç§°::[è¾“å…¥å½¢çŠ¶]_x_[æƒé‡å½¢çŠ¶]
        results[f"RMSNorm_Input::[{M//tp}, {H}]_x_[{H}]"] = norm_time
        results[f"Attn_QKV::[{M}, {H}]_x_[{3*H//tp}, {H}]"] = qkv_time
        results[f"Attn_O::[{M}, {H//tp}]_x_[{H}, {H//tp}]"] = o_time
        results[f"RMSNorm_PostAttn::[{M//tp}, {H}]_x_[{H}]"] = norm_time
        results[f"MLP_GateUp::[{M}, {H}]_x_[{2*FFN//tp}, {H}]"] = gate_up_time
        results[f"SiLU::[{M}, {2*FFN//tp}]_x_None"] = silu_time
        results[f"MLP_Down::[{M}, {FFN//tp}]_x_[{H}, {FFN//tp}]"] = down_time
        results[f"Sum_Parts_TP{tp}"] = sum_of_parts
        results[f"LAYER_REAL_TP{tp}"] = simulated_layer_time
        
    out_file = os.path.join(os.getcwd(), "benchmark_layer_times.json")
    try:
        with open(out_file, "w") as f:
            json.dump(results, f, indent=4)
        print(f"[*] Successfully saved benchmark profiling stats to {out_file}\n")
    except Exception as e:
        print(f"[!] Failed to write benchmark stats: {e}\n")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--S', type=int, default=4096, help="Sequence Length")
    parser.add_argument('--B', type=int, default=1, help="Micro Batch Size")
    parser.add_argument('--H', type=int, default=4096, help="Hidden Size")
    parser.add_argument('--FFN', type=int, default=11008, help="FFN Intermediate Size")
    args = parser.parse_args()
    
    device_name = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"[*] Detected hardware device: {device_name.upper()}")
    if device_name != 'cuda':
        print("[!] è­¦å‘Š: æœ¬åœ°è¿è¡Œä»…æµ‹è¯•ä»£ç é€»è¾‘ã€‚æ¬²è§‚æµ‹çœŸå®çš„ TensorCore éçº¿æ€§æŠ˜æŸï¼Œè¯·å°†æ­¤è„šæœ¬æäº¤è‡³ A40/A100 GPU é›†ç¾¤è¿è¡Œï¼\n")
        
    profile_llama_layer(S=args.S, B=args.B, H=args.H, FFN=args.FFN, tp_sizes=[1, 2, 4, 8])
